{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=6, micro=1, releaselevel='final', serial=0)\n",
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import sys\n",
    "print(sys.version_info)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sin(n_samples=400, n_discr_points = 10):\n",
    "    x = np.arange(start=0.0, stop=2.0*np.pi, step=2.0*np.pi/n_discr_points)\n",
    "    omega = np.random.uniform(1.0, 4.0, (n_samples, x.shape[0]) )\n",
    "    return np.sin(omega*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sum(n_samples=600, n_discr_points=5):\n",
    "    Y = np.random.randint(low=0,high=2,size=(n_samples,n_discr_points-1))\n",
    "    Y = np.c_[Y, np.sum(Y, axis=1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 8)\n"
     ]
    }
   ],
   "source": [
    "n_samples = 600\n",
    "n_discr_points = 8\n",
    "y = make_sum(n_samples, n_discr_points)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402, 8)\n",
      "(198, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y_train, y_test = train_test_split( y, test_size=0.33, random_state=42 )\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_neurons = [8,5]\n",
    "n_hidden_layers = len(n_hidden_neurons)\n",
    "nlayers = n_hidden_layers+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unroll_factor = n_discr_points - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanSquareError():\n",
    "        \n",
    "    def error(self, y, a):\n",
    "        return (a-y)**2\n",
    "    \n",
    "    def derror(self,a, y):\n",
    "        return 2*(a-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InputLayer():\n",
    "    def __init__(self, n=2):\n",
    "        self.z = np.zeros(shape=(n,1))\n",
    "        self.N = n\n",
    "    def forward(self, x):\n",
    "        self.z = np.array(x).reshape(self.N, 1)\n",
    "        return x\n",
    "    def last_activ(self):\n",
    "        return self.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OutputLayer():\n",
    "    def __init__(self, n=1, nprev=5, cost=MeanSquareError() ):\n",
    "        self.N = n\n",
    "        self.Nprev = nprev\n",
    "        \n",
    "        self.z = np.zeros(shape=(n,1))\n",
    "        \n",
    "        self.b = np.random.uniform(low=0., high=1., size=(n,1))\n",
    "        self.bupdates = np.zeros_like(self.b)\n",
    "        \n",
    "        # W represents the matrix of weights from the PREVIOUS layer to THIS layer\n",
    "        self.W = np.random.uniform(low=0., high=1., size=(n,nprev))\n",
    "        self.Wupdates = np.zeros_like(self.W)\n",
    "        \n",
    "        self.cost_ = cost\n",
    "        \n",
    "    def forward(self, x, tt=0):\n",
    "        self.z = (self.W @ x).reshape(self.N,1) + self.b\n",
    "        return self.z\n",
    "    \n",
    "    def last_activ(self):\n",
    "        return self.z\n",
    "    \n",
    "    def backward(self, y):\n",
    "        return self.cost_.derror(self.z,y)\n",
    "    \n",
    "    def update(self):\n",
    "        self.b -= self.bupdates\n",
    "        self.W -= self.Wupdates\n",
    "        self.bupdates = np.zeros_like(self.b)\n",
    "        self.Wupdates = np.zeros_like(self.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecurrentGRULayer():\n",
    "    def __init__(self, n=5, nprev=5, unroll_fac=1):\n",
    "        self.N = n\n",
    "        self.Nprev = nprev\n",
    "        self.unroll_ = unroll_fac\n",
    "        \n",
    "        # there are (unroll_fac + 1) values because of the fake initialized values\n",
    "        self.z = np.zeros(shape=(n,1,unroll_fac+1))\n",
    "        self.r = np.zeros(shape=(n,1,unroll_fac+1))\n",
    "        self.g = np.zeros(shape=(n,1,unroll_fac+1))\n",
    "        self.h = np.random.uniform(low=0., high=1., size=(n,1,unroll_fac+1) )\n",
    "        \n",
    "        self.Wz = np.random.uniform(low=0., high=1., size=(n,n))\n",
    "        self.Uz = np.random.uniform(low=0., high=1., size=(n,nprev))\n",
    "        self.Wr = np.random.uniform(low=0., high=1., size=(n,n))\n",
    "        self.Ur = np.random.uniform(low=0., high=1., size=(n,nprev))\n",
    "        self.Wg = np.random.uniform(low=0., high=1., size=(n,n))\n",
    "        self.Ug = np.random.uniform(low=0., high=1., size=(n,nprev))\n",
    "        \n",
    "        self.Wzupdates = np.zeros_like(self.Wz)\n",
    "        self.Uzupdates = np.zeros_like(self.Uz)\n",
    "        self.Wrupdates = np.zeros_like(self.Wr)\n",
    "        self.Urupdates = np.zeros_like(self.Ur)\n",
    "        self.Wgupdates = np.zeros_like(self.Wg)\n",
    "        self.Ugupdates = np.zeros_like(self.Ug)\n",
    "        \n",
    "        self.deltah = np.zeros_like(self.h)\n",
    "        \n",
    "        \n",
    "    def re_init(self):        \n",
    "        self.z = np.zeros(shape=(self.N,1,self.unroll_+1))\n",
    "        self.r = np.zeros(shape=(self.N,1,self.unroll_+1))\n",
    "        self.g = np.zeros(shape=(self.N,1,self.unroll_+1))\n",
    "        self.h = np.random.uniform(low=0., high=1., size=(self.N,1,self.unroll_+1) )\n",
    "        \n",
    "    def forward(self, x, tt):\n",
    "        \n",
    "        x = np.array([x])\n",
    "        x = x.reshape(self.Nprev, 1)\n",
    "        \n",
    "        self.z[:,:,tt+1] = sigmoid( self.Wz @ self.h[:,:,tt] + self.Uz @ x)\n",
    "        self.r[:,:,tt+1] = sigmoid( self.Wr @ self.h[:,:,tt] + self.Ur @ x)\n",
    "        self.g[:,:,tt+1] = np.tanh( self.Wg @ (self.r[:,:,tt+1] * self.h[:,:,tt] )\\\n",
    "                                    + self.Ug @ x)\n",
    "        self.h[:,:,tt+1] = ( 1. - self.z[:,:,tt+1] ) * self.h[:,:,tt] \\\n",
    "                              + self.z[:,:,tt+1] * self.g[:,:,tt+1]\n",
    "        \n",
    "        return self.h[:,:,tt+1]\n",
    "    \n",
    "    def last_activ(self):\n",
    "        return self.h[:,:,-1]\n",
    "    \n",
    "    def backward(self, tt, deltaX):\n",
    "        \"\"\"backward returns intermediate deltas and total deltas\"\"\"\n",
    "        delta = deltaX + self.deltah[:,:,tt+1]\n",
    "        delta = delta.reshape(self.N, 1)\n",
    "        delta_g = delta*self.z[:,:,tt+1]*(1. - self.g[:,:,tt+1]**2)\n",
    "        delta_r = delta_g*(self.Wg.T @ self.h[:,:,tt])*\\\n",
    "                             self.r[:,:,tt+1]*(1. - self.r[:,:,tt+1])\n",
    "        delta_z = delta*(self.g[:,:,tt+1] - self.h[:,:,tt])*\\\n",
    "                      self.z[:,:,tt+1]*(1. - self.z[:,:,tt+1])\n",
    "        delta_x = self.Ug.T @ delta_g +\\\n",
    "                  self.Ur.T @ delta_r +\\\n",
    "                  self.Uz.T @ delta_z\n",
    "        self.deltah[:,:,tt] = self.Wg.T @ delta_g +\\\n",
    "                  self.Wr.T @ delta_r +\\\n",
    "                  self.Wz.T @ delta_z\n",
    "        return delta_g, delta_r, delta_z, delta_x                         \n",
    "    \n",
    "    def update(self):\n",
    "        self.Wz -= self.Wzupdates\n",
    "        self.Uz -= self.Uzupdates\n",
    "        self.Wg -= self.Wgupdates\n",
    "        self.Ug -= self.Ugupdates\n",
    "        self.Wr -= self.Wrupdates\n",
    "        self.Ur -= self.Urupdates\n",
    "        self.Wzupdates = np.zeros_like(self.Wz)\n",
    "        self.Uzupdates = np.zeros_like(self.Uz)\n",
    "        self.Wrupdates = np.zeros_like(self.Wr)\n",
    "        self.Urupdates = np.zeros_like(self.Ur)\n",
    "        self.Wgupdates = np.zeros_like(self.Wg)\n",
    "        self.Ugupdates = np.zeros_like(self.Ug)\n",
    "        self.deltah = np.zeros_like(self.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## TEST GRU forward\n",
    "gru = RecurrentGRULayer(3,2,2)\n",
    "gru2 = RecurrentGRULayer(3,3,2)\n",
    "x_in = np.array([0.5, 0.5]).reshape(1,2)\n",
    "for t in range(2):\n",
    "    h = gru.forward(x_in,t)\n",
    "    h2 = gru2.forward(h,t)\n",
    "## TEST GRU backward\n",
    "for t in range(1,-1,-1):\n",
    "    print(t)\n",
    "    delta_x = np.array([0.1,0.5,0.1]).reshape(3,1)\n",
    "    delta_g, delta_r, delta_z, delta_x = gru2.backward(t,delta_x)\n",
    "    gru2.Wgupdates += np.outer(delta_g, gru2.h[:,:,t])\n",
    "    gru2.Wrupdates += np.outer(delta_r, gru2.h[:,:,t])\n",
    "    gru2.Wzupdates += np.outer(delta_z, gru2.h[:,:,t])\n",
    "    gru2.Ugupdates += np.outer(delta_g, gru.h[:,:,t+1])\n",
    "    gru2.Urupdates += np.outer(delta_r, gru.h[:,:,t+1])\n",
    "    gru2.Uzupdates += np.outer(delta_z, gru.h[:,:,t+1])\n",
    "    delta_g, delta_r, delta_z, delta_x = gru.backward(t,delta_x)\n",
    "    gru.Wgupdates += np.outer(delta_g, gru.h[:,:,t])\n",
    "    gru.Wrupdates += np.outer(delta_r, gru.h[:,:,t])\n",
    "    gru.Wzupdates += np.outer(delta_z, gru.h[:,:,t])\n",
    "    gru.Ugupdates += np.outer(delta_g, x_in)\n",
    "    gru.Urupdates += np.outer(delta_r, x_in)\n",
    "    gru.Uzupdates += np.outer(delta_z, x_in)\n",
    "    \n",
    "gru2.update()\n",
    "gru.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CostFunction = MeanSquareError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "layers.append( InputLayer(1) )\n",
    "nprev = 1\n",
    "for n in n_hidden_neurons:\n",
    "    layers.append(RecurrentGRULayer(n, nprev, unroll_factor))\n",
    "    nprev = n\n",
    "layers.append(OutputLayer(1,nprev,CostFunction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "n_epochs = 10\n",
    "\n",
    "training_error = []\n",
    "validation_error = []\n",
    "    \n",
    "for i in range(n_epochs):\n",
    "\n",
    "    errors = []\n",
    "    for idx, data_sample in enumerate(y_train):\n",
    "        \n",
    "        for l in range(1,nlayers-1):\n",
    "            layers[l].re_init()\n",
    "        \n",
    "        # feedforward\n",
    "        for t in range(unroll_factor):\n",
    "            a = layers[0].forward(data_sample[t])\n",
    "            for i in range(1,nlayers):\n",
    "                a = layers[i].forward(a,t)\n",
    "\n",
    "        # now a holds the prediction for the next value\n",
    "        # the true next value is data_sample[unroll_factor]\n",
    "        errors.append(CostFunction.error(data_sample[unroll_factor], a))\n",
    "    \n",
    "        ###########################################\n",
    "        # backprop through time\n",
    "        \n",
    "        # output layer: no time        \n",
    "        error = layers[nlayers-1].backward(data_sample[unroll_factor])\n",
    "        top_error = error        \n",
    "        layers[nlayers-1].Wupdates = \\\n",
    "            alpha*np.outer(error, layers[nlayers-2].last_activ() )\n",
    "        layers[nlayers-1].bupdates = alpha*error    \n",
    "        \n",
    "        # hidden layers: back through time\n",
    "        # but this is the last instant, so don't compute Whh update\n",
    "        delta_x = error\n",
    "        for i in range(nlayers-2,1,-1):\n",
    "            delta_g, delta_r, delta_z, delta_x = layers[i].backward(t,delta_x)\n",
    "            # in the semantics I chose, layers[i].h[:,:,t] corresponds to \n",
    "            # the activation at time (t-1)\n",
    "            layers[i].Wgupdates += np.outer(delta_g, layers[i].h[:,:,0])\n",
    "            layers[i].Wrupdates += np.outer(delta_r, layers[i].h[:,:,0]) \n",
    "            layers[i].Wzupdates += np.outer(delta_z, layers[i].h[:,:,0])\n",
    "            layers[i].Ugupdates += np.outer(delta_g, layers[i-1].h[:,:,1])\n",
    "            layers[i].Urupdates += np.outer(delta_r, layers[i-1].h[:,:,1])\n",
    "            layers[i].Uzupdates += np.outer(delta_z, layers[i-1].h[:,:,1])\n",
    "            \n",
    "        # first hidden layer: back through time \n",
    "        # but this is the last instant, so don't compute Whh update\n",
    "        delta_g, delta_r, delta_z, delta_x = layers[1].backward(t,delta_x)\n",
    "        # in the semantics I chose, layers[i].h[:,:,t] corresponds to \n",
    "        # the activation at time (t-1)\n",
    "        layers[1].Wgupdates += np.outer(delta_g, layers[1].h[:,:,0])\n",
    "        layers[1].Wrupdates += np.outer(delta_r, layers[1].h[:,:,0]) \n",
    "        layers[1].Wzupdates += np.outer(delta_z, layers[1].h[:,:,0])\n",
    "        layers[1].Ugupdates += np.outer(delta_g, layers[0].last_activ())\n",
    "        layers[1].Urupdates += np.outer(delta_r, layers[0].last_activ())\n",
    "        layers[1].Uzupdates += np.outer(delta_z, layers[0].last_activ())\n",
    "\n",
    "        \n",
    "        for t in range(unroll_factor-2,-1,-1):\n",
    "            \n",
    "            delta_x = np.zeros_like(top_error)\n",
    "       \n",
    "            for i in range(nlayers-2,1,-1):\n",
    "                delta_g, delta_r, delta_z, delta_x = layers[i].backward(t,delta_x)\n",
    "                # in the semantics I chose, layers[i].h[:,:,t] corresponds to \n",
    "                # the activation at time (t-1)\n",
    "                layers[i].Wgupdates += np.outer(delta_g, layers[i].h[:,:,t])\n",
    "                layers[i].Wrupdates += np.outer(delta_r, layers[i].h[:,:,t]) \n",
    "                layers[i].Wzupdates += np.outer(delta_z, layers[i].h[:,:,t])\n",
    "                layers[i].Ugupdates += np.outer(delta_g, layers[i-1].h[:,:,t+1])\n",
    "                layers[i].Urupdates += np.outer(delta_r, layers[i-1].h[:,:,t+1])\n",
    "                layers[i].Uzupdates += np.outer(delta_z, layers[i-1].h[:,:,t+1])\n",
    "\n",
    "                \n",
    "            # first hidden layer: back through time            \n",
    "            delta_g, delta_r, delta_z, delta_x = layers[1].backward(t,delta_x)\n",
    "            # in the semantics I chose, layers[i].h[:,:,t] corresponds to \n",
    "            # the activation at time (t-1)\n",
    "            layers[1].Wgupdates += np.outer(delta_g, layers[1].h[:,:,t])\n",
    "            layers[1].Wrupdates += np.outer(delta_r, layers[1].h[:,:,t]) \n",
    "            layers[1].Wzupdates += np.outer(delta_z, layers[1].h[:,:,t])\n",
    "            layers[1].Ugupdates += np.outer(delta_g, layers[0].last_activ())\n",
    "            layers[1].Urupdates += np.outer(delta_r, layers[0].last_activ())\n",
    "            layers[1].Uzupdates += np.outer(delta_z, layers[0].last_activ())\n",
    "\n",
    "        for i in range(1,nlayers):\n",
    "            layers[i].update()\n",
    "            \n",
    "    training_error.append(np.mean(errors))\n",
    "    \n",
    "    # validation\n",
    "    errors = []\n",
    "    for idx, data_sample in enumerate(y_test):\n",
    "        # feedforward\n",
    "        for t in range(unroll_factor):\n",
    "            a = layers[0].forward(data_sample[t])\n",
    "            for i in range(1,nlayers):\n",
    "                a = layers[i].forward(a,t)\n",
    "        errors.append(CostFunction.error(data_sample[unroll_factor], a[0]))\n",
    "    validation_error.append(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ffd78072780>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAKvCAYAAAB+sZnuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsw3uRL2LAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffd368aa860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(range(n_epochs), training_error, '.-')\n",
    "plt.plot(range(n_epochs), validation_error, '.-')\n",
    "plt.legend(['training','validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(layers, data_sample):\n",
    "    # feedforward\n",
    "    for t in range(unroll_factor):\n",
    "        a = layers[0].forward(data_sample[t])\n",
    "        for i in range(1,nlayers):\n",
    "            a = layers[i].forward(a,t)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    print(y_test[i,:])\n",
    "    print(inference(layers,y_test[i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
