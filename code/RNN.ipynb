{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very Simple Recurrent Neural Network\n",
    "\n",
    "This network can be used to demonstrate how to implement the backprop algorithm in a Recurrent Neural Network (RNN).\n",
    "\n",
    "This notebook represents the code associated with the writeup on [my blog](LINK).\n",
    "\n",
    "The structure of the network is **many-to-one** as described in C. Olah's [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "The network has:\n",
    "1. an input layer\n",
    "2. an arbitrary number of recurrent, fully connected sigmoid layers\n",
    "3. an output layer.\n",
    "\n",
    "The purpose of the network is to predict the last element of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "print(sys.version_info)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General structure\n",
    "\n",
    "I am going to define a the network at the layer-level.\n",
    "This means that there will be a python class for each layer.\n",
    "\n",
    "Each layer must contain at least:\n",
    "1. knowledge of how many neurons it has (`N`);\n",
    "2. knowledge of how many neurons were in the previous layer (`Nprev`);\n",
    "2. knowledge of how far back in time the network should unrolled (`unroll_fac`);\n",
    "2. a matrix of hidden states of size `N`-by-`unroll_fac+1`, where the $t^{th}$ row represents the activation at time $t-1$ (`h`);\n",
    "2. a matrix `W` of weights, of size `N`-by-`Nprev`;\n",
    "2. a matrix `Whh` of recurrent weights, of size `N`-by-`N`;\n",
    "2. a vector `b` of biases, of size `N`;\n",
    "3. a forward method that takes a vector of size `Nprev` as input, and the returns the vector of activations of size `N`;\n",
    "3. a backward method that computes the error signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Input layer\n",
    "\n",
    "The input layer takes a vector of size `n`, and outputs the exact same vector, but also stores it in an internal history.\n",
    "As of right now, this is not the most useful of layers.\n",
    "But in principle one could benefit from this infrastructure to implement some pre-processing (e.g. creating new features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecurrentInputLayer():\n",
    "    \"\"\"Designed to be compatible with Recurrent layers\"\"\"\n",
    "    def __init__(self, n=2, unroll_fac=1):\n",
    "        self.N = n\n",
    "        self.unroll_ = unroll_fac\n",
    "        self.h = np.zeros(shape=(n,self.unroll_+1))\n",
    "    def forward(self, x, tt):\n",
    "        self.h[:,tt+1] = np.array(x).reshape(self.N, 1)\n",
    "        return self.h[:,tt+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Layer\n",
    "\n",
    "all the *hidden* layers of this network are going to be of this type.\n",
    "The activation function is the sigmoid, defined by\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forward pass:\n",
    "\n",
    "Since we are using the formulation that includes both weights and biases, this is how the forward activation of the layer will be computed:\n",
    "\n",
    "1. for every timestep $n = 1, \\dots,  N$:\n",
    "  - for every layer $l = 1, \\dots, L$:\n",
    "    - compute the weighted input\n",
    "$$ \\mathbf{z}_l^n = W_l \\mathbf{h}_{l-1}^n + W_{hh} \\mathbf{h}_{l}^{n-1} + b_l^n $$\n",
    "    - compute the activation\n",
    "$$ \\mathbf{h}_l^n = \\mathbf{\\sigma} ( \\mathbf{z}_l^n ) $$\n",
    "\n",
    "#### backward pass:\n",
    "\n",
    "The backprop algorithm for RNN requires two error signals: $\\sideset{_S}{^n_l}\\delta$ in *space* (layers) and $\\sideset{_T}{^n_l}\\delta$ in *time*.\n",
    "\n",
    "In the **many-to-one** architecture, the error on the cost function is evaluated only **once** at the very end of the feedforward pass.\n",
    "The backprop algorithm can then be written as:\n",
    "\n",
    "1. compute the gradient of the cost function, evaluated on the activations of the last layer at the last timestep;\n",
    "2. set the *error signal*\n",
    "$$ \\sideset{_C}{}\\delta = \\nabla C \\Bigg|_{\\mathbf{h_L^N}} $$\n",
    "3. for each layer $ l $ going backwards:\n",
    "  - for each time step $n$ going backwards:\n",
    "    - compute the total error signal\n",
    "$$ \\delta_l^n = \\sideset{_S}{^n_{l+1}}\\delta + \\sideset{_T}{^{n+1}_l}\\delta $$\n",
    "    - update the weight matrix\n",
    "$$ W_l \\leftarrow W_l - \\eta \\left ( \\delta \\odot \\frac{ d \\sigma }{dz} \\Bigg|_{\\mathbf{z}_{l}^n} \\right ) \\otimes \\mathbf{h}_{l-1}^n  $$\n",
    "    - update the recurrent weight matrix\n",
    "$$ W_{hh} \\leftarrow W_{hh} - \\eta \\left ( \\delta \\odot \\frac{ d \\sigma }{dz} \\Bigg|_{\\mathbf{z}_{l}^n} \\right ) \\otimes \\mathbf{h}_{l}^{n-1}  $$\n",
    "    - update the biases\n",
    "$$ \\mathbf{b}_l \\leftarrow \\mathbf{b}_l - \\eta \\left ( \\delta \\odot \\frac{ d \\sigma }{dz} \\Bigg|_{\\mathbf{z}_{l}^n} \\right ) $$\n",
    "    - backpropagate the space error signal\n",
    "$$ \\sideset{_S}{^n_{l}}\\delta = W_l^T \\left ( \\delta \\odot \\frac{ d \\sigma }{dz} \\Bigg|_{\\mathbf{z}_{l}^n} \\right ) $$\n",
    "    - backpropagate the time error signal\n",
    "$$ \\sideset{_T}{^n_{l}}\\delta = W_{hh}^T \\left ( \\delta \\odot \\frac{ d \\sigma }{dz} \\Bigg|_{\\mathbf{z}_{l}^n} \\right ) $$\n",
    "\n",
    "\n",
    "The `backward` method of the layer has three responsibilities:\n",
    "- compute the update factor\n",
    "$$ \\left (  \\left ( \\sideset{_S}{^n_{l+1}}\\delta + \\sideset{_T}{^{n+1}_l}\\delta \\right ) \\odot \\frac{ d \\sigma }{dz} \\Bigg|_{\\mathbf{z}_{l}} \\right ) $$\n",
    "- compute the space error signal to be backpropagated\n",
    "$$ \\sideset{_S}{^n_{l}}\\delta = W_l^T \\left ( \\left ( \\sideset{_S}{^n_{l+1}}\\delta + \\sideset{_T}{^{n+1}_l}\\delta \\right ) \\odot \\frac{ d \\sigma }{dz} \\Bigg|_{\\mathbf{z}_{l}} \\right ) $$\n",
    "- compute the time error signal to be backpropagated\n",
    "$$ \\sideset{_T}{^n_{l}}\\delta = W_{hh}^T \\left ( \\left ( \\sideset{_S}{^n_{l+1}}\\delta + \\sideset{_T}{^{n+1}_l}\\delta \\right ) \\odot \\frac{ d \\sigma }{dz} \\Bigg|_{\\mathbf{z}_{l}} \\right ) $$\n",
    "\n",
    "Remember, the update factor for layer $l$ has size $N_l$, whereas the space error signal has size $N_{l-1}$ and the time error signal has size $N_l$.\n",
    "\n",
    "###### important note\n",
    "\n",
    "Troughout this notebook I am going to use the fact that\n",
    "\n",
    "$ \\frac{d\\sigma}{dz} \\Big|_{z_0} = \\sigma(z_0)( 1 - \\sigma(z_0) ) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecurrentSigmoidLayer():\n",
    "    def __init__(self, n=5, nprev=5, unroll_fac=1):\n",
    "        self.N = n\n",
    "        self.Nprev = nprev\n",
    "        self.unroll_ = unroll_fac\n",
    "        \n",
    "        self.h = np.random.uniform(low=0., high=1., size=(self.N,self.unroll_+1))\n",
    "        \n",
    "        self.b = np.random.uniform(low=0., high=1., size=(self.N,1))\n",
    "        self.bupdates = np.zeros_like(self.b)\n",
    "        \n",
    "        # W represents the matrix of weights from the PREVIOUS layer to THIS layer\n",
    "        self.W = np.random.uniform(low=0., high=1., size=(self.N,self.Nprev))\n",
    "        self.Wupdates = np.zeros_like(self.W)\n",
    "        \n",
    "        self.Whh = np.random.uniform(low=0., high=1., size=(self.N,self.N))\n",
    "        self.Whhupdates = np.zeros_like(self.Whh)\n",
    "        \n",
    "    def forward(self, x, tt):\n",
    "        \n",
    "        x = np.array([x])\n",
    "        x = x.reshape(self.Nprev, 1)\n",
    "        \n",
    "        z = self.W @ x + self.Whh @ self.h[:,tt].reshape(self.N,1) #+ self.b       \n",
    "        \n",
    "        self.h[:,tt+1] = sigmoid(z).reshape(self.N,)     \n",
    "   \n",
    "        return self.h[:,tt+1]\n",
    "    \n",
    "    def backward(self, Sdelta, Tdelta, tt):\n",
    "\n",
    "        delta = Sdelta + Tdelta\n",
    "\n",
    "        h = self.h[:,tt+1].reshape(self.N,1)\n",
    "        \n",
    "        update_fac = delta * h * (1. - h)\n",
    "\n",
    "        new_Sdelta = self.W.T @ update_fac\n",
    "\n",
    "        new_Tdelta = self.Whh.T @ update_fac  \n",
    "\n",
    "        return update_fac, new_Sdelta, new_Tdelta\n",
    "\n",
    "\n",
    "    \n",
    "    def update(self):\n",
    "        self.b -= self.bupdates\n",
    "        self.W -= self.Wupdates\n",
    "        self.Whh -= self.Whhupdates\n",
    "        self.bupdates = np.zeros_like(self.b)\n",
    "        self.Wupdates = np.zeros_like(self.W)        \n",
    "        self.Whhupdates = np.zeros_like(self.Whh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer\n",
    "\n",
    "is a bit more complicated than the input layer.\n",
    "First of all, the output layer has knowledge of the cost function. \n",
    "Additionally, the `backward` method takes the *true label* as input.\n",
    "\n",
    "Moreover, since our method will be required to predict arbitrary numbers (see below where we generate the data), we use a simple *linear* activation function to avoid clipping output results to a given value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "We are going to use [least-squares](https://en.wikipedia.org/wiki/Least_squares), a popular cost function.\n",
    "\n",
    "For a single datapoint, if we define $y$ as the true label (which can take any real value) and $p$ as the predicted value by our model, we have the cost function defined by\n",
    "\n",
    "$ (p - y)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanSquareError():\n",
    "        \n",
    "    def error(self, a, y):\n",
    "        return (a-y)**2\n",
    "    \n",
    "    def derror(self,a, y):\n",
    "        return 2*(a-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ManyToOneOutputLayer():\n",
    "    def __init__(self, n=1, nprev=5, cost=MeanSquareError() ):\n",
    "        self.N = n\n",
    "        self.Nprev = nprev\n",
    "        \n",
    "        self.h = np.zeros(shape=(self.N,1))\n",
    "        \n",
    "        self.b = np.random.uniform(low=0., high=1., size=(self.N,1))\n",
    "        self.bupdates = np.zeros_like(self.b)\n",
    "        \n",
    "        self.W = np.random.uniform(low=0., high=1., size=(self.N,self.Nprev))\n",
    "        self.Wupdates = np.zeros_like(self.W)\n",
    "        \n",
    "        self.cost_ = cost\n",
    "        \n",
    "    def forward(self, x, tt=0):\n",
    "        x = np.array([x])\n",
    "        x = x.reshape(self.Nprev,1)        \n",
    "        \n",
    "        self.h = (self.W @ x).reshape(self.N,1) + self.b\n",
    "\n",
    "        return self.h\n",
    "    \n",
    "    def backward(self, y):\n",
    "        update_fac = self.cost_.derror(self.h,y)\n",
    "        new_delta = self.W.T @ update_fac\n",
    "\n",
    "        return update_fac, new_delta      \n",
    "    \n",
    "    def update(self):\n",
    "        self.b -= self.bupdates\n",
    "        self.W -= self.Wupdates\n",
    "        self.bupdates = np.zeros_like(self.b)\n",
    "        self.Wupdates = np.zeros_like(self.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some fake sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum of binary sequence\n",
    "\n",
    "The fake data we are going to generate has the following structure: a sequence of 1's and 0's, plus a last value that represents the sum of all the values in the sequence.\n",
    "\n",
    "An example of a sequence with 5 points would be\n",
    "\n",
    "$$ 1, 0, 1, 1, 3 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sum(n_samples=600, n_discr_points=5):\n",
    "    Y = np.random.randint(low=0,high=2,size=(n_samples,n_discr_points-1))\n",
    "    Y = np.c_[Y, np.sum(Y, axis=1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_simpler(n_samples=600, n_discr_points=5):\n",
    "    Y = np.random.randint(low=0,high=2,size=(n_samples,n_discr_points-1))\n",
    "    Y = np.c_[Y, Y[:,-1]]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 600\n",
    "n_discr_points = 8\n",
    "y = make_sum(n_samples, n_discr_points)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_neurons = [6,4]\n",
    "n_hidden_layers = len(n_hidden_neurons)\n",
    "nlayers = n_hidden_layers+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unroll_factor = n_discr_points - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CostFunction = MeanSquareError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = []\n",
    "layers.append( RecurrentInputLayer(1,unroll_factor) )\n",
    "nprev = 1\n",
    "for n in n_hidden_neurons:\n",
    "    layers.append(RecurrentSigmoidLayer(n, nprev, unroll_factor))\n",
    "    nprev = n\n",
    "layers.append(ManyToOneOutputLayer(1,nprev,CostFunction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validate\n",
    "\n",
    "A common practice is to split the dataset in training and validation.\n",
    "This is done to try to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y_train, y_test = train_test_split( y, test_size=0.33, random_state=42 )\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "n_epochs = 100\n",
    "\n",
    "training_error = []\n",
    "validation_error = []\n",
    "    \n",
    "for i in range(n_epochs):\n",
    "\n",
    "    errors = []\n",
    "    for idx, data_sample in enumerate(y_train):\n",
    "        \n",
    "        # feedforward\n",
    "        for t in range(unroll_factor):\n",
    "            a = data_sample[t]\n",
    "            for i in range(0,nlayers):\n",
    "                a = layers[i].forward(a,t)\n",
    "\n",
    "        # now a holds the prediction for the next value\n",
    "        # the true next value is data_sample[unroll_factor]\n",
    "        errors.append(CostFunction.error(a, data_sample[unroll_factor]))\n",
    "    \n",
    "        ###########################################\n",
    "        # backprop through time\n",
    "        \n",
    "        # output layer: no time        \n",
    "        update_fac, delta = layers[nlayers-1].backward(data_sample[unroll_factor])     \n",
    "        layers[nlayers-1].Wupdates = eta*np.outer(update_fac, layers[nlayers-2].h[:,-1] )\n",
    "        layers[nlayers-1].bupdates = eta*update_fac   \n",
    "        \n",
    "        # Sdelta_dict[l][t] is the error signal that layer l passes to layer (l-1) at time t\n",
    "        # Tdelta_dict[l][t] is the error signal that layer l passes to itself from time t to t-1\n",
    "        Sdelta_dict = dict()\n",
    "        Tdelta_dict = dict()\n",
    "        for l in range(nlayers-1,0,-1):\n",
    "            Sdelta_dict[l] = dict()\n",
    "            Tdelta_dict[l] = dict()\n",
    "            for t in range(unroll_factor,-1,-1):\n",
    "                Sdelta_dict[l][t] = np.zeros( shape=(layers[l].Nprev,1) )\n",
    "                Tdelta_dict[l][t] = np.zeros( shape=(layers[l].N,1) )\n",
    "\n",
    "        Sdelta_dict[nlayers-1][unroll_factor-1] = delta.reshape(layers[nlayers-1].Nprev,1)\n",
    "        \n",
    "        for l in range(nlayers-2,0,-1):\n",
    "            for t in range(unroll_factor-1,-1,-1):\n",
    "                update_fac, Sdelta, Tdelta = layers[l].backward(Sdelta_dict[l+1][t], Tdelta_dict[l][t+1], t)\n",
    "                Sdelta_dict[l][t] = Sdelta\n",
    "                Tdelta_dict[l][t] = Tdelta\n",
    "                layers[l].Wupdates   += eta*np.outer(update_fac, layers[l-1].h[:,t+1] )\n",
    "                layers[l].Whhupdates += eta*np.outer(update_fac, layers[l].h[:,t] )\n",
    "                layers[l].bupdates   += eta*update_fac\n",
    "                   \n",
    "        for i in range(1,nlayers):\n",
    "            layers[i].update()\n",
    "            \n",
    "    training_error.append(np.mean(errors))\n",
    "    \n",
    "    # validation\n",
    "    errors = []\n",
    "    for idx, data_sample in enumerate(y_test):\n",
    "        # feedforward\n",
    "        for t in range(unroll_factor):\n",
    "            a = data_sample[t]\n",
    "            for i in range(0,nlayers):\n",
    "                a = layers[i].forward(a,t)\n",
    "        errors.append(CostFunction.error(a, data_sample[unroll_factor]))\n",
    "    validation_error.append(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(range(n_epochs), training_error, '.-')\n",
    "plt.plot(range(n_epochs), validation_error, '.-')\n",
    "plt.legend(['training','validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time\n",
    "\n",
    "Let's do some inference!\n",
    "This means trying to predict the class of new data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(layers[-2].Whh)\n",
    "plt.colorbar()\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(layers[-2].W)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(layers[-3].Whh)\n",
    "plt.colorbar()\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(layers[-3].W)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(layers, data_sample):\n",
    "    # feedforward\n",
    "    for t,a in enumerate(data_sample[:-1]):\n",
    "        for i in range(0,nlayers):\n",
    "            a = layers[i].forward(a,t)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    print(y_test[i,:])\n",
    "    print(inference(layers,y_test[i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arbitrary_Y = np.random.randint(low=0,high=2,size=(1,n_discr_points-1))\n",
    "arbitrary_Y = np.c_[arbitrary_Y, np.sum(arbitrary_Y, axis=1)]\n",
    "arbitrary_Y = arbitrary_Y.reshape(n_discr_points,)\n",
    "print(arbitrary_Y)\n",
    "print(inference(layers, arbitrary_Y.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
