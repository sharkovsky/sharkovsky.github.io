{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import sys\n",
    "print(sys.version_info)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sin(n_samples=400, n_discr_points = 10):\n",
    "    x = np.arange(start=0.0, stop=2.0*np.pi, step=2.0*np.pi/n_discr_points)\n",
    "    omega = np.random.uniform(1.0, 4.0, (n_samples, x.shape[0]) )\n",
    "    return np.sin(omega*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sum(n_samples=600, n_discr_points=5):\n",
    "    Y = np.random.randint(low=0,high=2,size=(n_samples,n_discr_points-1))\n",
    "    Y = np.c_[Y, np.sum(Y, axis=1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 600\n",
    "n_discr_points = 8\n",
    "y = make_sum(n_samples, n_discr_points)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y_train, y_test = train_test_split( y, test_size=0.33, random_state=42 )\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_neurons = [8,5]\n",
    "n_hidden_layers = len(n_hidden_neurons)\n",
    "nlayers = n_hidden_layers+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unroll_factor = n_discr_points - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanSquareError():\n",
    "        \n",
    "    def error(self, y, a):\n",
    "        return (a-y)**2\n",
    "    \n",
    "    def derror(self,a, y):\n",
    "        return 2*(a-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InputLayer():\n",
    "    def __init__(self, n=2):\n",
    "        self.z = np.zeros(shape=(n,1))\n",
    "        self.N = n\n",
    "    def forward(self, x):\n",
    "        self.z = np.array(x).reshape(self.N, 1)\n",
    "        return x\n",
    "    def last_activ(self):\n",
    "        return self.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OutputLayer():\n",
    "    def __init__(self, n=1, nprev=5, cost=MeanSquareError() ):\n",
    "        self.N = n\n",
    "        self.Nprev = nprev\n",
    "        \n",
    "        self.z = np.zeros(shape=(n,1))\n",
    "        \n",
    "        self.b = np.random.uniform(low=0., high=1., size=(n,1))\n",
    "        self.bupdates = np.zeros_like(self.b)\n",
    "        \n",
    "        # W represents the matrix of weights from the PREVIOUS layer to THIS layer\n",
    "        self.W = np.random.uniform(low=0., high=1., size=(n,nprev))\n",
    "        self.Wupdates = np.zeros_like(self.W)\n",
    "        \n",
    "        self.cost_ = cost\n",
    "        \n",
    "    def forward(self, x, tt=0):\n",
    "        self.z = (self.W @ x).reshape(self.N,1) + self.b\n",
    "        return self.z\n",
    "    \n",
    "    def last_activ(self):\n",
    "        return self.z\n",
    "    \n",
    "    def backward(self, y):\n",
    "        return self.cost_.derror(self.z,y)\n",
    "    \n",
    "    def update(self):\n",
    "        self.b -= self.bupdates\n",
    "        self.W -= self.Wupdates\n",
    "        self.bupdates = np.zeros_like(self.b)\n",
    "        self.Wupdates = np.zeros_like(self.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecurrentSigmoidLayer():\n",
    "    def __init__(self, n=5, nprev=5, unroll_fac=1):\n",
    "        self.N = n\n",
    "        self.Nprev = nprev\n",
    "        self.unroll_ = unroll_fac\n",
    "        \n",
    "        self.z = np.zeros(shape=(n,1))\n",
    "        self.activations = np.zeros(shape=(n, self.unroll_))\n",
    "        \n",
    "        self.b = np.random.uniform(low=0., high=1., size=(n,1))\n",
    "        self.bupdates = np.zeros_like(self.b)\n",
    "        \n",
    "        # W represents the matrix of weights from the PREVIOUS layer to THIS layer\n",
    "        self.W = np.random.uniform(low=0., high=1., size=(n,nprev))\n",
    "        self.Wupdates = np.zeros_like(self.W)\n",
    "        \n",
    "        self.Whh = np.random.uniform(low=0., high=1., size=(n,n))\n",
    "        self.Whhupdates = np.zeros_like(self.Whh)\n",
    "        \n",
    "    def forward(self, x, tt):\n",
    "        if np.isscalar(x):\n",
    "            self.z = np.dot(self.W, x).reshape(self.N,1) + np.dot(self.Whh, sigmoid(self.z)) + self.b\n",
    "        else:\n",
    "            self.z = (self.W @ x).reshape(self.N,1) + (self.Whh @ sigmoid(self.z)) + self.b               \n",
    "   \n",
    "        self.activations[:,tt] = sigmoid(self.z).reshape(self.N,)\n",
    "        return sigmoid(self.z)\n",
    "    \n",
    "    def last_activ(self):\n",
    "        return sigmoid(self.z)\n",
    "    \n",
    "    def backward(self, W, dLdz):\n",
    "        '''Note: this W is NOT self.W, but the one from the downstream layer!'''\n",
    "        return (W.T @ dLdz)*sigmoid(self.z)*(1. - sigmoid(self.z))\n",
    "                \n",
    "    \n",
    "    def update(self):\n",
    "        self.b -= self.bupdates\n",
    "        self.W -= self.Wupdates\n",
    "        self.Whh -= self.Whhupdates\n",
    "        self.bupdates = np.zeros_like(self.b)\n",
    "        self.Wupdates = np.zeros_like(self.W)        \n",
    "        self.Whhupdates = np.zeros_like(self.Whh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CostFunction = MeanSquareError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = []\n",
    "layers.append( InputLayer(1) )\n",
    "nprev = 1\n",
    "for n in n_hidden_neurons:\n",
    "    layers.append(RecurrentSigmoidLayer(n, nprev, unroll_factor))\n",
    "    nprev = n\n",
    "layers.append(OutputLayer(1,nprev,CostFunction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.005\n",
    "n_epochs = 400\n",
    "\n",
    "training_error = []\n",
    "validation_error = []\n",
    "    \n",
    "for i in range(n_epochs):\n",
    "\n",
    "    errors = []\n",
    "    for idx, data_sample in enumerate(y_train):\n",
    "        \n",
    "        # feedforward\n",
    "        for t in range(unroll_factor):\n",
    "            a = layers[0].forward(data_sample[t])\n",
    "            for i in range(1,nlayers):\n",
    "                a = layers[i].forward(a,t)\n",
    "\n",
    "        # now a holds the prediction for the next value\n",
    "        # the true next value is data_sample[unroll_factor]\n",
    "        errors.append(CostFunction.error(data_sample[unroll_factor], a))\n",
    "    \n",
    "        ###########################################\n",
    "        # backprop through time\n",
    "        \n",
    "        # output layer: no time        \n",
    "        error = layers[nlayers-1].backward(data_sample[unroll_factor])\n",
    "        top_error = error        \n",
    "        layers[nlayers-1].Wupdates = \\\n",
    "            alpha*np.outer(error, layers[nlayers-2].last_activ() )\n",
    "        layers[nlayers-1].bupdates = alpha*error    \n",
    "        \n",
    "        # hidden layers: back through time\n",
    "        # but this is the last instant, so don't compute Whh update\n",
    "        for i in range(nlayers-2,1,-1):\n",
    "            error = layers[i].backward(layers[i+1].W, error)\n",
    "            layers[i].Wupdates += alpha*np.outer(error , layers[i-1].activations[:,-1])\n",
    "            layers[i].bupdates += alpha*error\n",
    "            \n",
    "        # first hidden layer: back through time \n",
    "        # but this is the last instant, so don't compute Whh update\n",
    "        error = layers[1].backward(layers[2].W, error)\n",
    "        layers[1].Wupdates += alpha*np.outer(error , layers[0].last_activ())\n",
    "        layers[1].bupdates += alpha*error\n",
    "\n",
    "        \n",
    "        for t in range(unroll_factor-2,-1,-1):\n",
    "            \n",
    "            error = top_error\n",
    "       \n",
    "            for i in range(nlayers-2,1,-1):\n",
    "                error = layers[i].backward(layers[i+1].W, error)\n",
    "                layers[i].Wupdates += \\\n",
    "                           alpha*np.outer(error , layers[i-1].activations[:,t])\n",
    "                layers[i].Whhupdates += \\\n",
    "                           alpha*np.outer(error , layers[i].activations[:,t+1])\n",
    "                layers[i].bupdates += alpha*error\n",
    "                \n",
    "            # first hidden layer: back through time            \n",
    "            error = layers[1].backward(layers[2].W, error)\n",
    "            layers[1].Wupdates += alpha*np.outer(error , layers[0].last_activ())\n",
    "            layers[1].Whhupdates += \\\n",
    "                           alpha*np.outer(error , layers[1].activations[:,t+1])\n",
    "            layers[1].bupdates += alpha*error\n",
    "\n",
    "        for i in range(1,nlayers):\n",
    "            layers[i].update()\n",
    "            \n",
    "    training_error.append(np.mean(errors))\n",
    "    \n",
    "    # validation\n",
    "    errors = []\n",
    "    for idx, data_sample in enumerate(y_test):\n",
    "        # feedforward\n",
    "        for t in range(unroll_factor):\n",
    "            a = layers[0].forward(data_sample[t])\n",
    "            for i in range(1,nlayers):\n",
    "                a = layers[i].forward(a,t)\n",
    "        errors.append(CostFunction.error(data_sample[unroll_factor], a[0]))\n",
    "    validation_error.append(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(range(n_epochs), training_error, '.-')\n",
    "plt.plot(range(n_epochs), validation_error, '.-')\n",
    "plt.legend(['training','validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(layers, data_sample):\n",
    "    # feedforward\n",
    "    for t in range(unroll_factor):\n",
    "        a = layers[0].forward(data_sample[t])\n",
    "        for i in range(1,nlayers):\n",
    "            a = layers[i].forward(a,t)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    print(y_test[i,:])\n",
    "    print(inference(layers,y_test[i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
